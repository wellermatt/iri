```{r echo=FALSE,warning=FALSE,message=FALSE}

  library("xtable")
  source("../.Rprofile")
  setwd(pth.dropbox.code)   ;     source("./results/00_results_load_prepare.R")
  
  

  #$$y_i = \alpha + \beta x_i + e_i$$
```

Model Selection and Sales Forecasting Hierarchies in Multi-Retailer Supply Chains with Promotions
========================================================

To clarify the research objectives, the experiment will focus on the manufacturer's sales forecasting forecasting problem at different levels of aggregation with the additional availability of store-level explanatory variables.  

The literature on hierarchical forecasting is limited, in particular amongst studies supported with empirical evidence.  Forecasting studies using POS data have commonly suffered from some combination of drawbacks: single-retailer, single-category and single-manufacturer models with demand processes which do not represent reality (i.e. without price & promotional effects) are typically used.  The forecasting methods used have tended to be MSE, moving averages and exponential smooothing.  Whilst univariate methods are proven to work well at the monthly level, their application to weekly data with its camplex calendar & promotional effects is less well documented.

Initial tests have been based on a single SKU, sold through 60 stores and 6 chains.  The weekly sales data and explanatory variables have been aggregated temporally and hierarchically via a calendar mapping (weeks to 445 periods) and a forecasting hierarchy of 3 levels: 
* Item (UPC/SKU)
* Retailer (Chain/Account)
* Store (POS/outlet)

The experiment seeks to evaluate alternative forecasting methods at different levels of aggregation to test the multivariate models against the univariate benchmarks.  Data is split into a 48:24 month fit:holdout (208:105 weeks) and rolling origin forecasts are made for the 3-month/13-week horizon.

Forecasting methods included in for comparison are currently limited to Hyndman's ets package (in R) for state space versions of the smoothing family and regression with explanatory variables.  The regression follows the stepAIC function which selects the model based on the lowest AIC.  There may be possibilities to tweak the metaparameters which control addition and removal of variables during the process.

The variables available are numerous and include:

1. Promotional Dummies
 * FEATURE (A+/A/B/C)
 * DISPLAY (major/minor)
2. Price
 * Average selling price 
 * derived variables REFERENCE_PRICE, DISCOUNT
 * differencing and log transforms
3. Holidays & Special Events: 10 holidays
4. Combinations of above variables
5. Lags/Lead of the explanatory variables

Data
----

Overview of the dataset

Item selection and missing data, consideration of temporary SKUs and supersession

Plots of the item at each level: weekly and 445

Split the data into fit and test periods


The model
--------

Aggregation of weekly data: temporal and hierarchical

Forecasting methods: ets (exponential smoothing) and regression (stepAIC)

Rolling origin forecasts generated for weekly and monthly series

Weekly forecasts aggregated to monthly via simple summation

Monthly forecasts disaggregated using identical weights for each week within a period.

Results
----

The results for comparison are weekly forecasts made at each level for 68 time series (60 stores, 7 chains, 1 item).  We use a monthly cycle of forecasting to follow the 445 calendar as this is the most common frequency for S&OP cycles.  As such we discard weekly forecasts which are not made at the 445 period end.

Error measures calculated for each point forecast include: error, absolute error, relative error, relative absolute error, scaled error?

Plots of individual forecasts: weekly and monthly at each level

```{r echo=FALSE,warning=FALSE,message=FALSE, fig.height=6, fig.width=12, results='asis'}

# get the results for both of weekly/445 time buckets  
  fcast.comp.dat = rbindlist(list(f_get.fcast.comp.dat.weekly() [k<14],f_get.fcast.comp.dat.445()))  # load the results of regression v ets
  fcast.comp.dat[,Level:=factor(lvl,labels = c("ITEM", "CHAIN", "STORE"))]
  print(xtable(dcast(fcast.comp.dat, formula=Level+periodicity+method~k,median,na.rm=TRUE,value.var="rae")),
        include.rownames = F, type="html")


  print(xtable(dcast(fcast.comp.dat,formula=method~Level+periodicity,median,na.rm=TRUE,value.var="rae")), type="html")
  
  p = ggplot(fcast.comp.dat, aes(x=factor(k),y = rae, colour = method, shape = method, size=1)) +  
      stat_summary(fun.y = median,na.rm=TRUE,geom="point") + 
      facet_grid(Level~periodicity)+scale_y_continuous(limits = c(0,0.5)) +
      theme_bw() + coord_flip()
  print(p)

```

```{r echo=FALSE,warning=FALSE,message=FALSE, fig.height=4, fig.width=8, results='asis'}
   p = ggplot(fcast.comp.dat, aes(x=method, y = rae, colour = method)) + geom_boxplot() +
    theme_bw() +facet_grid(Level~periodicity)+scale_y_continuous(limits = c(0,2))+coord_flip()
  print(p)
```



```{r fig.width=7, fig.height=6,results='asis'}
print(xtable(head(fcast.comp.dat)),type="html")
```

